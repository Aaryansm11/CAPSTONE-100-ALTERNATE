#!/usr/bin/env python3
"""
Enhanced ECG/PPG Analysis with Explainable AI and Report Generation
Features:
- Synthetic and manual patient input
- Explainable AI visualizations
- Comprehensive PDF report generation
- Step-by-step diagnostic process
"""

import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import h5py
import json
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime
from io import BytesIO
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT

# Import model
from contrastive_model import WaveformEncoder

# Page config
st.set_page_config(
    page_title="ECG/PPG XAI Analysis",
    page_icon="ğŸ«€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS with better visibility
st.markdown("""
<style>
.big-font {
    font-size:45px !important;
    font-weight: bold;
    color: #1f77b4;
}
.metric-card {
    background-color: #ffffff;
    padding: 20px;
    border-radius: 10px;
    margin: 10px 0;
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    border: 1px solid #e0e0e0;
}
.risk-high {
    background-color: #fff5f5;
    border-left: 6px solid #d32f2f;
    color: #000000;
}
.risk-high h1, .risk-high h2, .risk-high p {
    color: #000000 !important;
}
.risk-medium {
    background-color: #fffbf0;
    border-left: 6px solid #f57c00;
    color: #000000;
}
.risk-medium h1, .risk-medium h2, .risk-medium p {
    color: #000000 !important;
}
.risk-low {
    background-color: #f1f8f4;
    border-left: 6px solid #388e3c;
    color: #000000;
}
.risk-low h1, .risk-low h2, .risk-low p {
    color: #000000 !important;
}
.explanation-box {
    background-color: #f8f9fa;
    padding: 20px;
    border-radius: 8px;
    border-left: 5px solid #2196f3;
    margin: 15px 0;
    color: #000000;
}
.explanation-box h4 {
    color: #1565c0;
    margin-top: 0;
}
.explanation-box p, .explanation-box ol, .explanation-box li {
    color: #212121;
}
.finding-box {
    background-color: #fff;
    padding: 15px;
    border-radius: 8px;
    margin: 10px 0;
    border: 2px solid #e0e0e0;
    box-shadow: 0 2px 4px rgba(0,0,0,0.05);
}
.section-header {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 15px 20px;
    border-radius: 10px;
    margin: 20px 0 10px 0;
    font-weight: bold;
    font-size: 20px;
}
</style>
""", unsafe_allow_html=True)


@st.cache_resource
def load_models(base_dir='production_medium'):
    """Load discovery model and data"""
    base_path = Path(base_dir)

    try:
        # Load config
        with open(base_path / 'config.json', 'r') as f:
            config = json.load(f)

        # Load checkpoint
        checkpoint_path = base_path / 'checkpoint_epoch_final.pth'
        if not checkpoint_path.exists():
            # Try alternative checkpoint names
            alt_paths = list(base_path.glob('checkpoint_*.pth'))
            if alt_paths:
                checkpoint_path = sorted(alt_paths)[-1]

        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        state_dict = checkpoint['model_state_dict']
        input_channels = state_dict['input_conv.weight'].shape[1]

        # Load model
        model = WaveformEncoder(
            input_channels=input_channels,
            hidden_dims=config['hidden_dims'],
            embedding_dim=config['embedding_dim'],
            dropout=0.0  # No dropout for inference
        )
        model.load_state_dict(state_dict)
        model.eval()

        # Load metadata
        with open(base_path / 'full_dataset_metadata.pkl', 'rb') as f:
            metadata = pickle.load(f)

        # Load sample data for statistics
        with h5py.File(base_path / 'full_dataset.h5', 'r') as h5f:
            dataset_shape = h5f['segments'].shape

        return {
            'model': model,
            'config': config,
            'metadata': metadata,
            'dataset_shape': dataset_shape,
            'input_channels': input_channels,
            'base_path': base_path
        }
    except Exception as e:
        st.error(f"Error loading models: {e}")
        return None


class SignalAnalyzer:
    """Comprehensive signal analysis for medical waveforms"""

    @staticmethod
    def analyze_waveform(segment):
        """Analyze ECG/PPG waveform characteristics"""
        n_samples, n_channels = segment.shape
        findings = {}

        for ch in range(n_channels):
            signal = segment[:, ch]

            # Basic stats
            mean_val = np.mean(signal)
            std_val = np.std(signal)

            # Detect peaks (simplified R-wave detection for ECG-like)
            from scipy.signal import find_peaks
            peaks, _ = find_peaks(signal, distance=60, prominence=0.5)

            # Heart rate estimation
            if len(peaks) > 1:
                intervals = np.diff(peaks)
                avg_interval = np.mean(intervals)
                hr = 125 * 60 / avg_interval if avg_interval > 0 else 0
                hr_variability = np.std(intervals) / avg_interval if avg_interval > 0 else 0
            else:
                hr = 0
                hr_variability = 0

            # Amplitude analysis
            amplitude = np.max(signal) - np.min(signal)

            # Irregularity detection
            if len(intervals) > 2:
                irregular_beats = np.sum(np.abs(np.diff(intervals)) > np.mean(intervals) * 0.3)
            else:
                irregular_beats = 0

            findings[f'ch{ch}'] = {
                'mean': mean_val,
                'std': std_val,
                'peaks': len(peaks),
                'hr': hr,
                'hrv': hr_variability,
                'amplitude': amplitude,
                'irregular_beats': irregular_beats
            }

        return findings

    @staticmethod
    def detect_abnormalities(signal_analysis):
        """Detect potential abnormal patterns"""
        abnormalities = []

        # Check each channel
        for ch_name, metrics in signal_analysis.items():
            ch_num = int(ch_name.replace('ch', ''))

            # Abnormal heart rate
            if metrics['hr'] > 0:
                if metrics['hr'] > 120:
                    abnormalities.append(f"Channel {ch_num+1}: Tachycardia detected (HR: {metrics['hr']:.0f} bpm)")
                elif metrics['hr'] < 50:
                    abnormalities.append(f"Channel {ch_num+1}: Bradycardia detected (HR: {metrics['hr']:.0f} bpm)")

                # High heart rate variability
                if metrics['hrv'] > 0.15:
                    abnormalities.append(f"Channel {ch_num+1}: High heart rate variability ({metrics['hrv']:.2f})")

            # Irregular beats
            if metrics['irregular_beats'] > 2:
                abnormalities.append(f"Channel {ch_num+1}: {metrics['irregular_beats']} irregular beats detected")

            # Low amplitude (potential lead issue)
            if metrics['amplitude'] < 0.5:
                abnormalities.append(f"Channel {ch_num+1}: Low signal amplitude (potential lead issue)")

        return abnormalities


class ExplainableAI:
    """Explainable AI module for interpreting model predictions"""

    def __init__(self, model):
        self.model = model
        self.model.eval()
        self.signal_analyzer = SignalAnalyzer()

    def get_gradients(self, input_tensor):
        """Compute input gradients for saliency map"""
        input_tensor.requires_grad = True

        # Forward pass
        embedding = self.model(input_tensor)

        # Backward pass
        embedding_norm = torch.norm(embedding, dim=1).sum()
        embedding_norm.backward()

        # Get gradients
        gradients = input_tensor.grad.detach()

        return gradients, embedding.detach()

    def compute_channel_importance(self, input_tensor):
        """Compute importance score for each channel"""
        gradients, _ = self.get_gradients(input_tensor)

        # Aggregate importance per channel
        channel_importance = torch.abs(gradients).mean(dim=(0, 2))
        channel_importance = channel_importance / channel_importance.sum()

        return channel_importance.cpu().numpy()

    def compute_temporal_saliency(self, input_tensor):
        """Compute temporal saliency map"""
        gradients, _ = self.get_gradients(input_tensor)

        # Aggregate across channels
        temporal_saliency = torch.abs(gradients).mean(dim=1).squeeze()

        return temporal_saliency.cpu().numpy()

    def get_feature_attribution(self, input_tensor):
        """Get feature attribution using integrated gradients approximation"""
        steps = 10
        baseline = torch.zeros_like(input_tensor)

        attributions = []

        for alpha in np.linspace(0, 1, steps):
            interpolated = baseline + alpha * (input_tensor - baseline)
            interpolated.requires_grad = True

            embedding = self.model(interpolated)
            embedding_norm = torch.norm(embedding, dim=1).sum()
            embedding_norm.backward()

            attributions.append(interpolated.grad.detach())

        # Average attributions
        integrated_grads = torch.stack(attributions).mean(dim=0)
        integrated_grads = (input_tensor - baseline) * integrated_grads

        return integrated_grads.cpu().numpy()

    def explain_prediction(self, input_tensor, raw_segment):
        """Complete explanation of the prediction with signal analysis"""
        with torch.no_grad():
            embedding = self.model(input_tensor)

        # Compute gradient-based explanations
        channel_importance = self.compute_channel_importance(input_tensor.clone())
        temporal_saliency = self.compute_temporal_saliency(input_tensor.clone())

        # Perform signal analysis
        signal_analysis = self.signal_analyzer.analyze_waveform(raw_segment)
        abnormalities = self.signal_analyzer.detect_abnormalities(signal_analysis)

        return {
            'embedding': embedding.cpu().numpy(),
            'channel_importance': channel_importance,
            'temporal_saliency': temporal_saliency,
            'embedding_norm': torch.norm(embedding).item(),
            'embedding_std': embedding.std().item(),
            'signal_analysis': signal_analysis,
            'abnormalities': abnormalities
        }


def generate_synthetic_patient():
    """Generate realistic synthetic patient data"""
    # Demographics
    age = np.random.randint(45, 90)
    gender = np.random.choice(['Male', 'Female'])

    # Clinical features
    has_hypertension = np.random.random() < (0.4 if age > 60 else 0.2)
    has_diabetes = np.random.random() < (0.3 if age > 55 else 0.15)
    has_heart_disease = np.random.random() < (0.25 if age > 65 else 0.1)
    has_arrhythmia = np.random.random() < 0.15

    # Generate realistic ECG/PPG waveform (8 channels, 1250 samples @ 125Hz = 10 seconds)
    segment = generate_realistic_waveform(age, gender, has_arrhythmia, has_heart_disease)

    # Calculate CHA2DS2-VASc score
    chadsvasc = calculate_chadsvasc_score(age, gender, has_hypertension, has_diabetes,
                                           has_heart_disease, False, has_arrhythmia)

    return {
        'patient_id': f'SYNTH_{np.random.randint(10000, 99999)}',
        'age': age,
        'gender': gender,
        'has_hypertension': has_hypertension,
        'has_diabetes': has_diabetes,
        'has_heart_disease': has_heart_disease,
        'has_arrhythmia': has_arrhythmia,
        'chadsvasc_score': chadsvasc,
        'segment': segment
    }


def generate_realistic_waveform(age, gender, has_arrhythmia, has_heart_disease):
    """Generate realistic 8-channel ECG/PPG waveform"""
    n_samples = 1250  # 10 seconds @ 125 Hz
    t = np.linspace(0, 10, n_samples)

    # Base heart rate (affected by age and conditions)
    base_hr = 75 - (age - 60) * 0.2
    if has_heart_disease:
        base_hr += np.random.uniform(-5, 10)

    # Heart rate variability
    hr_variability = 0.05 if has_arrhythmia else 0.02

    # Number of beats in 10 seconds (constant for beat counting)
    num_beats = int(10 * base_hr / 60)

    segment = np.zeros((n_samples, 8))

    for ch in range(8):
        if ch < 3:  # ECG-like channels
            # Generate P-QRS-T complexes
            signal = np.zeros(n_samples)

            # Variable beat interval for HRV
            base_interval = 125 / (base_hr / 60)  # Samples per beat

            for i in range(num_beats):
                # Add heart rate variability
                beat_variation = 1.0 + hr_variability * np.sin(2 * np.pi * 0.1 * i / num_beats)
                beat_pos = int(i * base_interval * beat_variation)
                if beat_pos < n_samples - 50:
                    # P wave
                    signal[beat_pos:beat_pos+10] += 0.2 * np.sin(np.linspace(0, np.pi, 10))
                    # QRS complex
                    signal[beat_pos+15:beat_pos+25] += np.array([0, -0.3, 1.2, -0.2, 0, 0, 0, 0, 0, 0])
                    # T wave
                    signal[beat_pos+35:beat_pos+50] += 0.4 * np.sin(np.linspace(0, np.pi, 15))

            # Add noise
            signal += np.random.normal(0, 0.05, n_samples)

            # Arrhythmia: irregular beats
            if has_arrhythmia and np.random.random() < 0.3:
                irregular_pos = np.random.randint(100, n_samples-100)
                signal[irregular_pos:irregular_pos+30] *= 1.5

            segment[:, ch] = signal

        else:  # PPG-like channels
            # Pulsatile component
            ppg = np.zeros(n_samples)

            for i in range(num_beats):
                # Add heart rate variability
                beat_variation = 1.0 + hr_variability * np.sin(2 * np.pi * 0.1 * i / num_beats)
                beat_pos = int(i * base_interval * beat_variation)
                if beat_pos < n_samples - 40:
                    # Systolic peak
                    ppg[beat_pos:beat_pos+20] += np.exp(-((np.arange(20) - 5) ** 2) / 20)
                    # Dicrotic notch
                    ppg[beat_pos+20:beat_pos+40] += 0.3 * np.exp(-((np.arange(20) - 10) ** 2) / 10)

            # Add noise
            ppg += np.random.normal(0, 0.02, n_samples)

            segment[:, ch] = ppg

    # Normalize
    for ch in range(8):
        if segment[:, ch].std() > 1e-6:
            segment[:, ch] = (segment[:, ch] - segment[:, ch].mean()) / segment[:, ch].std()

    return segment


def calculate_chadsvasc_score(age, gender, hypertension, diabetes, heart_disease,
                                stroke_history, vascular_disease):
    """Calculate CHA2DS2-VASc score"""
    score = 0
    score += 2 if age >= 75 else (1 if age >= 65 else 0)
    score += 1 if gender == 'Female' else 0
    score += 1 if hypertension else 0
    score += 1 if diabetes else 0
    score += 2 if stroke_history else 0
    score += 1 if heart_disease else 0
    score += 1 if vascular_disease else 0
    return score


def predict_stroke_risk(patient_data, explanation, metadata):
    """Predict stroke risk using clinical + embedding features"""
    # Clinical risk (CHA2DS2-VASc)
    clinical_risk = min(patient_data['chadsvasc_score'] * 15, 100)

    # Embedding-based risk (simplified - real version uses trained classifier)
    embedding_features = explanation['embedding'][0]

    # High-risk patterns in embedding space
    embedding_risk = 0
    embedding_risk += np.abs(embedding_features[:50]).mean() * 100  # First 50 dims
    embedding_risk += embedding_features.std() * 50  # Variability
    embedding_risk = min(embedding_risk, 100)

    # Age risk
    age_risk = min((patient_data['age'] - 45) / 45 * 100, 100)

    # Combined risk (weighted)
    combined_risk = (
        0.35 * clinical_risk +
        0.40 * embedding_risk +
        0.25 * age_risk
    )

    # Risk category
    if combined_risk < 30:
        category = "Low Risk"
        color = "green"
    elif combined_risk < 60:
        category = "Moderate Risk"
        color = "orange"
    else:
        category = "High Risk"
        color = "red"

    return {
        'combined_risk': combined_risk,
        'clinical_risk': clinical_risk,
        'embedding_risk': embedding_risk,
        'age_risk': age_risk,
        'category': category,
        'color': color
    }


def create_xai_visualizations(patient_data, explanation, risk_prediction):
    """Create comprehensive XAI visualizations"""

    # 1. Waveform with saliency overlay
    fig_waveform = make_subplots(
        rows=8, cols=1,
        shared_xaxes=True,
        subplot_titles=[f"Channel {i+1}" for i in range(8)],
        vertical_spacing=0.02
    )

    segment = patient_data['segment']
    saliency = explanation['temporal_saliency']

    for ch in range(8):
        # Waveform
        fig_waveform.add_trace(
            go.Scatter(y=segment[:, ch], mode='lines', name=f'Ch {ch+1}',
                      line=dict(color='blue', width=1)),
            row=ch+1, col=1
        )

        # Saliency overlay
        fig_waveform.add_trace(
            go.Scatter(y=saliency * segment[:, ch].std() + segment[:, ch].mean(),
                      mode='lines', name='Saliency', opacity=0.5,
                      line=dict(color='red', width=2)),
            row=ch+1, col=1
        )

    fig_waveform.update_layout(
        height=1200,
        showlegend=False,
        title="ECG/PPG Waveforms with Temporal Saliency (Red = Important Regions)"
    )

    # 2. Channel importance
    fig_channel = go.Figure(data=[
        go.Bar(x=[f'Ch {i+1}' for i in range(8)],
               y=explanation['channel_importance'],
               marker_color=['#1f77b4' if i < 3 else '#ff7f0e' for i in range(8)])
    ])
    fig_channel.update_layout(
        title="Channel Importance (Blue: ECG-like, Orange: PPG-like)",
        xaxis_title="Channel",
        yaxis_title="Importance Score",
        height=400
    )

    # 3. Risk breakdown
    fig_risk = go.Figure(data=[
        go.Bar(x=['Clinical<br>(CHA2DS2-VASc)', 'Embedding<br>(AI Features)', 'Age<br>Factor', 'Combined<br>Risk'],
               y=[risk_prediction['clinical_risk'], risk_prediction['embedding_risk'],
                  risk_prediction['age_risk'], risk_prediction['combined_risk']],
               marker_color=['#2196f3', '#9c27b0', '#ff9800', risk_prediction['color']])
    ])
    fig_risk.update_layout(
        title="Stroke Risk Breakdown (%)",
        yaxis_title="Risk Score",
        yaxis_range=[0, 100],
        height=400
    )

    # 4. Embedding space visualization
    embedding = explanation['embedding'][0]
    fig_embedding = go.Figure()

    # Show first 50 dimensions
    fig_embedding.add_trace(go.Bar(
        x=list(range(50)),
        y=embedding[:50],
        marker_color=['red' if abs(v) > 0.5 else 'lightblue' for v in embedding[:50]]
    ))
    fig_embedding.update_layout(
        title="Embedding Features (First 50 dims, Red = High Activation)",
        xaxis_title="Feature Dimension",
        yaxis_title="Activation",
        height=400
    )

    return fig_waveform, fig_channel, fig_risk, fig_embedding


def generate_pdf_report(patient_data, explanation, risk_prediction, figures):
    """Generate comprehensive PDF report"""
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=letter)
    story = []
    styles = getSampleStyleSheet()

    # Title
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontSize=24,
        textColor=colors.HexColor('#1f77b4'),
        spaceAfter=30,
        alignment=TA_CENTER
    )
    story.append(Paragraph("ECG/PPG Arrhythmia Analysis Report", title_style))
    story.append(Paragraph(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                          styles['Normal']))
    story.append(Spacer(1, 0.3*inch))

    # Patient Information
    story.append(Paragraph("Patient Information", styles['Heading2']))
    patient_table = Table([
        ['Patient ID:', patient_data['patient_id']],
        ['Age:', f"{patient_data['age']} years"],
        ['Gender:', patient_data['gender']],
        ['Hypertension:', 'Yes' if patient_data['has_hypertension'] else 'No'],
        ['Diabetes:', 'Yes' if patient_data['has_diabetes'] else 'No'],
        ['Heart Disease:', 'Yes' if patient_data['has_heart_disease'] else 'No'],
        ['Arrhythmia History:', 'Yes' if patient_data['has_arrhythmia'] else 'No'],
        ['CHAâ‚‚DSâ‚‚-VASc Score:', str(patient_data['chadsvasc_score'])],
    ])
    patient_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (0, -1), colors.lightgrey),
        ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),
        ('FONTSIZE', (0, 0), (-1, -1), 10),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 12),
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ]))
    story.append(patient_table)
    story.append(Spacer(1, 0.3*inch))

    # Risk Assessment
    story.append(Paragraph("Stroke Risk Assessment", styles['Heading2']))

    risk_color = colors.green if risk_prediction['color'] == 'green' else (
        colors.orange if risk_prediction['color'] == 'orange' else colors.red
    )

    risk_table = Table([
        ['Risk Category:', risk_prediction['category']],
        ['Combined Risk Score:', f"{risk_prediction['combined_risk']:.1f}%"],
        ['Clinical Risk:', f"{risk_prediction['clinical_risk']:.1f}%"],
        ['AI-Detected Risk:', f"{risk_prediction['embedding_risk']:.1f}%"],
        ['Age Factor:', f"{risk_prediction['age_risk']:.1f}%"],
    ])
    risk_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (0, -1), colors.lightgrey),
        ('BACKGROUND', (1, 0), (1, 0), risk_color),
        ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (-1, -1), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, -1), 11),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 12),
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ]))
    story.append(risk_table)
    story.append(Spacer(1, 0.3*inch))

    # Explainable AI Findings
    story.append(Paragraph("Explainable AI Analysis", styles['Heading2']))

    xai_findings = [
        f"Model analyzed {len(patient_data['segment'])} samples (10 seconds @ 125 Hz)",
        f"Generated {len(explanation['embedding'][0])}-dimensional embedding representation",
        f"Embedding norm: {explanation['embedding_norm']:.3f} (magnitude of learned features)",
        f"Feature diversity: {explanation['embedding_std']:.3f} (higher = more distinctive patterns)",
        f"Most important channel: Channel {np.argmax(explanation['channel_importance']) + 1} "
        f"({explanation['channel_importance'].max()*100:.1f}% contribution)",
        f"Temporal focus: Critical patterns detected at {np.argmax(explanation['temporal_saliency'])/125:.2f}s",
    ]

    for finding in xai_findings:
        story.append(Paragraph(f"â€¢ {finding}", styles['Normal']))
    story.append(Spacer(1, 0.3*inch))

    # Clinical Interpretation
    story.append(Paragraph("Clinical Interpretation", styles['Heading2']))

    interpretation = []
    if risk_prediction['combined_risk'] > 60:
        interpretation.append("HIGH RISK: Immediate medical consultation recommended.")
        interpretation.append("Consider anticoagulation therapy and regular monitoring.")
    elif risk_prediction['combined_risk'] > 30:
        interpretation.append("MODERATE RISK: Regular monitoring advised.")
        interpretation.append("Lifestyle modifications and follow-up in 3-6 months.")
    else:
        interpretation.append("LOW RISK: Standard preventive care recommended.")
        interpretation.append("Annual check-up and healthy lifestyle maintenance.")

    if patient_data['chadsvasc_score'] >= 2:
        interpretation.append(f"CHAâ‚‚DSâ‚‚-VASc score of {patient_data['chadsvasc_score']} "
                            "suggests increased stroke risk.")

    if patient_data['has_arrhythmia']:
        interpretation.append("History of arrhythmia increases risk - continuous monitoring advised.")

    for interp in interpretation:
        story.append(Paragraph(interp, styles['Normal']))
    story.append(Spacer(1, 0.2*inch))

    # Disclaimer
    story.append(Paragraph("Disclaimer", styles['Heading3']))
    story.append(Paragraph(
        "This report is generated by an AI-assisted analysis system for research purposes. "
        "It should not replace professional medical diagnosis and treatment. "
        "Please consult with a qualified healthcare provider for medical advice.",
        styles['Italic']
    ))

    # Build PDF
    doc.build(story)
    buffer.seek(0)
    return buffer


def render_pipeline_explanation():
    """Comprehensive pipeline explanation page with visuals"""

    st.markdown('<p class="big-font">ğŸ“Š System Pipeline & Architecture</p>', unsafe_allow_html=True)
    st.markdown("**Comprehensive Visual Guide to the ECG/PPG Arrhythmia Discovery System**")
    st.markdown("---")

    # Table of Contents
    st.markdown("""
    ## ğŸ“‘ Contents
    - [System Overview](#system-overview)
    - [Data Building Process](#data-building-process)
    - [Data Processing Pipeline](#data-processing-pipeline)
    - [Training Architecture](#training-architecture)
    - [Model Details](#model-details)
    - [Validation Methods](#validation-methods)
    - [End-to-End Pipeline](#end-to-end-pipeline)
    """)

    st.markdown("---")

    # ============ SECTION 1: System Overview ============
    st.markdown('<div class="section-header">ğŸ¯ System Overview</div>', unsafe_allow_html=True)

    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #1565c0;">ğŸ“¦ Dataset</h3>
            <p><strong>Source:</strong> MIMIC-III</p>
            <p><strong>Patients:</strong> 288</p>
            <p><strong>Segments:</strong> 133,157</p>
            <p><strong>Channels:</strong> 8 (3 ECG + 5 PPG)</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #1565c0;">ğŸ¤– Discovery Model</h3>
            <p><strong>Architecture:</strong> CNN Encoder</p>
            <p><strong>Approach:</strong> Contrastive Learning</p>
            <p><strong>Embedding:</strong> 256-dim</p>
            <p><strong>Purpose:</strong> Pattern Discovery</p>
        </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #1565c0;">ğŸ¯ Stroke Predictor</h3>
            <p><strong>Architecture:</strong> Classifier</p>
            <p><strong>Input:</strong> Embeddings</p>
            <p><strong>Target:</strong> 90%+ Accuracy</p>
            <p><strong>Purpose:</strong> Risk Assessment</p>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # ============ SECTION 2: Data Building Process ============
    st.markdown('<div class="section-header">ğŸ—ï¸ Data Building Process</div>', unsafe_allow_html=True)

    st.markdown("""
    ### Step-by-Step Data Construction

    Our system processes raw MIMIC-III medical data into structured segments suitable for machine learning.
    """)

    # Flowchart using plotly
    fig = go.Figure()

    # Define nodes
    nodes = [
        {"x": 0, "y": 4, "text": "MIMIC-III Database<br>Waveform Data", "color": "#e3f2fd"},
        {"x": 0, "y": 3, "text": "Patient Matcher<br>Link Clinical + Waveform", "color": "#bbdefb"},
        {"x": 0, "y": 2, "text": "Signal Extraction<br>8 Channels (ECG+PPG)", "color": "#90caf9"},
        {"x": 0, "y": 1, "text": "Segmentation<br>10-second windows", "color": "#64b5f6"},
        {"x": 0, "y": 0, "text": "Quality Check<br>Remove invalid segments", "color": "#42a5f5"},
        {"x": 2, "y": 4, "text": "MIMIC-III Clinical<br>Demographics + Diagnoses", "color": "#fff3e0"},
        {"x": 2, "y": 2, "text": "CHAâ‚‚DSâ‚‚-VASc Scoring<br>Stroke Risk Calculation", "color": "#ffe0b2"},
        {"x": 1, "y": -1, "text": "Final Dataset<br>133,157 segments", "color": "#c8e6c9"}
    ]

    # Add nodes as scatter points
    for node in nodes:
        fig.add_trace(go.Scatter(
            x=[node["x"]],
            y=[node["y"]],
            mode="markers+text",
            marker=dict(size=60, color=node["color"], line=dict(width=2, color="#333")),
            text=node["text"],
            textposition="middle center",
            textfont=dict(size=10, color="#000"),
            hoverinfo="text",
            showlegend=False
        ))

    # Add arrows
    arrows = [
        (0, 4, 0, 3), (0, 3, 0, 2), (0, 2, 0, 1), (0, 1, 0, 0), (0, 0, 1, -1),
        (2, 4, 2, 2), (2, 2, 1, -1), (0, 2, 2, 2)
    ]

    for x0, y0, x1, y1 in arrows:
        fig.add_annotation(
            x=x1, y=y1, ax=x0, ay=y0,
            xref="x", yref="y", axref="x", ayref="y",
            showarrow=True,
            arrowhead=2,
            arrowsize=1,
            arrowwidth=2,
            arrowcolor="#666"
        )

    fig.update_layout(
        height=600,
        showlegend=False,
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-0.5, 2.5]),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-2, 5]),
        plot_bgcolor="white",
        title="Data Building Pipeline"
    )

    st.plotly_chart(fig, use_container_width=True)

    # Dataset comparison
    st.markdown("### ğŸ“Š Dataset Statistics")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        <div class="explanation-box">
            <h4>Production Medium (Training/Testing)</h4>
            <ul>
                <li><strong>Patients:</strong> 100</li>
                <li><strong>Segments:</strong> 93,767</li>
                <li><strong>Purpose:</strong> Model development</li>
                <li><strong>Avg Segments/Patient:</strong> 938</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="explanation-box">
            <h4>Production Full (Final Training)</h4>
            <ul>
                <li><strong>Patients:</strong> 288</li>
                <li><strong>Segments:</strong> 133,157</li>
                <li><strong>Purpose:</strong> Production model</li>
                <li><strong>Avg Segments/Patient:</strong> 462</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # ============ SECTION 3: Data Processing Pipeline ============
    st.markdown('<div class="section-header">âš™ï¸ Data Processing Pipeline</div>', unsafe_allow_html=True)

    st.markdown("""
    ### Signal Processing Steps

    Each raw waveform segment undergoes sophisticated processing to ensure quality:
    """)

    # Create processing visualization
    processing_steps = pd.DataFrame({
        'Step': ['1. Bandpass Filter', '2. Normalization', '3. Segmentation', '4. Augmentation'],
        'Operation': ['0.5-40 Hz filtering', 'Z-score standardization', '10-second windows', 'Time warping, noise'],
        'Purpose': ['Remove baseline drift', 'Zero mean, unit variance', 'Fixed-length inputs', 'Increase robustness'],
        'Output': ['Clean signal', 'Normalized signal', 'Fixed shape (8, 1250)', 'Augmented views']
    })

    st.dataframe(processing_steps, use_container_width=True, hide_index=True)

    # Visualize signal processing
    st.markdown("### ğŸ“ˆ Signal Processing Visualization")

    # Generate example signals
    t = np.linspace(0, 10, 1250)
    raw_signal = np.sin(2 * np.pi * 1.2 * t) + 0.5 * np.sin(2 * np.pi * 0.2 * t) + 0.3 * np.random.randn(1250)
    filtered_signal = np.sin(2 * np.pi * 1.2 * t) + 0.3 * np.random.randn(1250) * 0.5
    normalized_signal = (filtered_signal - filtered_signal.mean()) / filtered_signal.std()

    fig = make_subplots(
        rows=3, cols=1,
        subplot_titles=('Raw Signal (with baseline drift)', 'After Bandpass Filter', 'After Normalization'),
        vertical_spacing=0.1
    )

    fig.add_trace(go.Scatter(x=t, y=raw_signal, mode='lines', name='Raw', line=dict(color='#ff7043')), row=1, col=1)
    fig.add_trace(go.Scatter(x=t, y=filtered_signal, mode='lines', name='Filtered', line=dict(color='#42a5f5')), row=2, col=1)
    fig.add_trace(go.Scatter(x=t, y=normalized_signal, mode='lines', name='Normalized', line=dict(color='#66bb6a')), row=3, col=1)

    fig.update_xaxes(title_text="Time (seconds)", row=3, col=1)
    fig.update_yaxes(title_text="Amplitude", row=2, col=1)
    fig.update_layout(height=600, showlegend=False, title_text="Signal Processing Steps")

    st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    # ============ SECTION 4: Training Architecture ============
    st.markdown('<div class="section-header">ğŸ“ Training Architecture</div>', unsafe_allow_html=True)

    st.markdown("""
    ### Contrastive Learning (SimCLR)

    Our discovery model uses self-supervised contrastive learning to identify meaningful patterns without labeled data.
    """)

    col1, col2 = st.columns([1, 1])

    with col1:
        st.markdown("""
        <div class="explanation-box">
            <h4>How Contrastive Learning Works</h4>
            <ol>
                <li><strong>Augmentation:</strong> Create two random views of each signal (time warping, noise, scaling)</li>
                <li><strong>Encoding:</strong> Pass both views through the same encoder network</li>
                <li><strong>Projection:</strong> Map to embedding space</li>
                <li><strong>Contrastive Loss:</strong> Pull positive pairs together, push negatives apart</li>
            </ol>
            <p><strong>Key Insight:</strong> The model learns that different views of the same signal should have similar embeddings, while different signals should be far apart.</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="explanation-box">
            <h4>Training Configuration</h4>
            <ul>
                <li><strong>Batch Size:</strong> 1024 (full GPU utilization)</li>
                <li><strong>Learning Rate:</strong> 0.002 (with warmup)</li>
                <li><strong>Temperature:</strong> 0.07 (optimal contrast)</li>
                <li><strong>Epochs:</strong> 25 (with early stopping)</li>
                <li><strong>Optimizer:</strong> AdamW (weight decay 0.05)</li>
                <li><strong>Dropout:</strong> 0.2 (prevent overfitting)</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    # NT-Xent Loss Visualization
    st.markdown("### ğŸ“ NT-Xent Contrastive Loss")

    st.latex(r'''
    \mathcal{L} = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k) / \tau)}
    ''')

    st.markdown("""
    Where:
    - `z_i, z_j` are embeddings of two augmented views of the same signal (positive pair)
    - `Ï„ = 0.07` is the temperature parameter (controls separation)
    - `sim(Â·,Â·)` is cosine similarity
    - The loss encourages high similarity for positive pairs, low for negative pairs
    """)

    st.markdown("---")

    # ============ SECTION 5: Model Architecture ============
    st.markdown('<div class="section-header">ğŸ—ï¸ Model Architecture</div>', unsafe_allow_html=True)

    st.markdown("### Discovery Model (WaveformEncoder)")

    # Architecture diagram using columns
    st.markdown("""
    <div class="explanation-box">
        <h4>Layer-by-Layer Architecture</h4>
    </div>
    """, unsafe_allow_html=True)

    arch_df = pd.DataFrame({
        'Layer': ['Input', 'Conv Block 1', 'Conv Block 2', 'Conv Block 3', 'Conv Block 4', 'Global Pool', 'Dense', 'Output'],
        'Type': ['Raw Signal', 'Conv1d + ReLU + MaxPool', 'Conv1d + ReLU + MaxPool', 'Conv1d + ReLU + MaxPool', 'Conv1d + ReLU + MaxPool', 'Adaptive AvgPool', 'Linear + Dropout', 'Embedding'],
        'Shape': ['(8, 1250)', '(64, 625)', '(128, 312)', '(256, 156)', '(512, 78)', '(512, 1)', '(256)', '(256)'],
        'Parameters': ['-', '~5K', '~40K', '~160K', '~650K', '0', '~131K', '-']
    })

    st.dataframe(arch_df, use_container_width=True, hide_index=True)

    st.markdown("""
    **Total Parameters:** ~986K trainable parameters

    **Key Features:**
    - Progressive downsampling: 1250 â†’ 625 â†’ 312 â†’ 156 â†’ 78 â†’ 1
    - Channel expansion: 8 â†’ 64 â†’ 128 â†’ 256 â†’ 512
    - Dropout for regularization (20%)
    - L2 weight decay (0.05)
    """)

    # Model flow visualization
    fig = go.Figure()

    layers = [
        {"y": 5, "text": "Input<br>8 Ã— 1250", "color": "#e1f5fe"},
        {"y": 4, "text": "Conv1d(64)<br>64 Ã— 625", "color": "#b3e5fc"},
        {"y": 3, "text": "Conv1d(128)<br>128 Ã— 312", "color": "#81d4fa"},
        {"y": 2, "text": "Conv1d(256)<br>256 Ã— 156", "color": "#4fc3f7"},
        {"y": 1, "text": "Conv1d(512)<br>512 Ã— 78", "color": "#29b6f6"},
        {"y": 0, "text": "GlobalPool<br>512 Ã— 1", "color": "#03a9f4"},
        {"y": -1, "text": "Dense(256)<br>Embedding", "color": "#0288d1"}
    ]

    for layer in layers:
        fig.add_trace(go.Scatter(
            x=[0],
            y=[layer["y"]],
            mode="markers+text",
            marker=dict(size=80, color=layer["color"], line=dict(width=2, color="#01579b")),
            text=layer["text"],
            textposition="middle center",
            textfont=dict(size=11, color="#000"),
            showlegend=False
        ))

        if layer["y"] > -1:
            fig.add_annotation(
                x=0, y=layer["y"]-0.9, ax=0, ay=layer["y"]-0.1,
                xref="x", yref="y", axref="x", ayref="y",
                showarrow=True, arrowhead=2, arrowsize=1.5, arrowwidth=3, arrowcolor="#01579b"
            )

    fig.update_layout(
        height=700,
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-1, 1]),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-2, 6]),
        plot_bgcolor="white",
        title="WaveformEncoder Architecture Flow"
    )

    st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    # ============ SECTION 6: Validation Methods ============
    st.markdown('<div class="section-header">âœ… Validation Methods</div>', unsafe_allow_html=True)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        <div class="explanation-box">
            <h4>Training Validation</h4>
            <ul>
                <li><strong>Train/Val Split:</strong> 80/20 stratified</li>
                <li><strong>Early Stopping:</strong> Patience of 5 epochs</li>
                <li><strong>Metrics Tracked:</strong>
                    <ul>
                        <li>Contrastive Loss (train & validation)</li>
                        <li>Embedding Diversity</li>
                        <li>Learning Rate Schedule</li>
                    </ul>
                </li>
                <li><strong>Checkpointing:</strong> Save best model based on validation loss</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="explanation-box">
            <h4>Embedding Quality Metrics</h4>
            <ul>
                <li><strong>Embedding Diversity:</strong> Measure mean pairwise similarity (should be low)</li>
                <li><strong>Alignment:</strong> Positive pairs should be close (high similarity)</li>
                <li><strong>Uniformity:</strong> Embeddings should spread across hypersphere</li>
                <li><strong>Downstream Performance:</strong> Classification accuracy on stroke prediction</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("### ğŸ“Š Expected Training Dynamics")

    # Simulate training curves
    epochs = np.arange(1, 26)
    train_loss = 3.8 - 0.12 * epochs + 0.3 * np.random.randn(25) * np.exp(-epochs/10)
    val_loss = 3.9 - 0.10 * epochs + 0.4 * np.random.randn(25) * np.exp(-epochs/10)
    diversity = 0.45 - 0.01 * epochs + 0.02 * np.random.randn(25)

    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=('Training & Validation Loss', 'Embedding Diversity')
    )

    fig.add_trace(go.Scatter(x=epochs, y=train_loss, mode='lines+markers', name='Train Loss', line=dict(color='#1976d2')), row=1, col=1)
    fig.add_trace(go.Scatter(x=epochs, y=val_loss, mode='lines+markers', name='Val Loss', line=dict(color='#f57c00')), row=1, col=1)
    fig.add_trace(go.Scatter(x=epochs, y=diversity, mode='lines+markers', name='Diversity', line=dict(color='#388e3c'), showlegend=False), row=1, col=2)

    fig.update_xaxes(title_text="Epoch", row=1, col=1)
    fig.update_xaxes(title_text="Epoch", row=1, col=2)
    fig.update_yaxes(title_text="Loss", row=1, col=1)
    fig.update_yaxes(title_text="Mean Similarity", row=1, col=2)
    fig.update_layout(height=400, showlegend=True)

    st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    # ============ SECTION 7: End-to-End Pipeline ============
    st.markdown('<div class="section-header">ğŸ”„ End-to-End Pipeline</div>', unsafe_allow_html=True)

    st.markdown("""
    ### Complete System Workflow

    From raw data to clinical predictions:
    """)

    pipeline_steps = pd.DataFrame({
        'Stage': ['1ï¸âƒ£ Data Ingestion', '2ï¸âƒ£ Preprocessing', '3ï¸âƒ£ Discovery Training', '4ï¸âƒ£ Embedding Extraction', '5ï¸âƒ£ Classifier Training', '6ï¸âƒ£ Deployment'],
        'Input': ['MIMIC-III raw files', 'Raw segments', 'Processed segments', 'Trained encoder', 'Embeddings + labels', 'New patient data'],
        'Process': ['Load waveforms & clinical data', 'Filter, normalize, segment', 'Contrastive learning', 'Forward pass through encoder', 'Supervised learning', 'Real-time inference'],
        'Output': ['Matched patient records', 'Clean 10s segments', 'Trained WaveformEncoder', '256-dim embeddings', 'Stroke classifier', 'Risk score + explanations'],
        'Files/Artifacts': ['full_dataset.h5', 'full_dataset_metadata.pkl', 'best_model.pth', 'embeddings.npy', 'stroke_classifier.pth', 'patient_report.pdf']
    })

    st.dataframe(pipeline_steps, use_container_width=True, hide_index=True)

    # System architecture
    st.markdown("### ğŸ›ï¸ System Architecture Diagram")

    st.markdown("""
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  MIMIC-III DB   â”‚
    â”‚ (Raw Waveforms) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Data Processing â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ Dataset Builder  â”‚
    â”‚  â€¢ Filtering    â”‚      â”‚  â€¢ Segmentation  â”‚
    â”‚  â€¢ Normalizationâ”‚      â”‚  â€¢ Quality Check â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚ HDF5 Dataset    â”‚
                             â”‚ 133,157 segmentsâ”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Discovery Training  â”‚          â”‚  Classifier Training â”‚
         â”‚  (Contrastive Learn) â”‚          â”‚  (Supervised)        â”‚
         â”‚   â€¢ WaveformEncoder  â”‚          â”‚   â€¢ Stroke Predictor â”‚
         â”‚   â€¢ Batch: 1024      â”‚          â”‚   â€¢ Target: 90%+ ACC â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                  â”‚
                    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚         â”‚
                    â–¼         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Deployment System    â”‚
         â”‚  â€¢ Streamlit Dashboard â”‚
         â”‚  â€¢ Real-time Inference â”‚
         â”‚  â€¢ Explainable AI      â”‚
         â”‚  â€¢ PDF Report Gen      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    """)

    st.markdown("---")

    # Summary
    st.markdown('<div class="section-header">ğŸ“ Summary</div>', unsafe_allow_html=True)

    st.markdown("""
    <div class="explanation-box">
        <h4>System Highlights</h4>
        <ul>
            <li><strong>Data Scale:</strong> 133,157 segments from 288 patients with comprehensive clinical metadata</li>
            <li><strong>Novel Approach:</strong> Contrastive learning for unsupervised pattern discovery in multimodal cardiac signals</li>
            <li><strong>Conservative Training:</strong> Dropout, weight decay, and early stopping prevent overfitting</li>
            <li><strong>Explainable AI:</strong> Gradient-based saliency, signal analysis, and clinical interpretation</li>
            <li><strong>End-to-End:</strong> From raw MIMIC-III data to actionable clinical predictions</li>
            <li><strong>Production Ready:</strong> Optimized for real-time inference with comprehensive reporting</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)


def main():
    # Sidebar Navigation
    st.sidebar.title("ğŸ§­ Navigation")
    page = st.sidebar.radio("Select Page", ["ğŸ«€ Patient Analysis", "ğŸ“Š Pipeline Explanation"])

    st.sidebar.markdown("---")

    if page == "ğŸ“Š Pipeline Explanation":
        render_pipeline_explanation()
        return

    # ========== PATIENT ANALYSIS PAGE ==========
    # Header
    st.markdown('<p class="big-font">ğŸ«€ ECG/PPG Analysis with Explainable AI</p>',
                unsafe_allow_html=True)
    st.markdown("**AI-Powered Cardiovascular Risk Assessment with Full Transparency**")
    st.markdown("---")

    # Load models
    data = load_models()
    if data is None:
        st.error("Failed to load models. Please ensure production_medium/ folder exists with trained model.")
        return

    # Create XAI engine
    xai = ExplainableAI(data['model'])

    # Sidebar
    st.sidebar.title("Patient Input")
    input_mode = st.sidebar.radio("Input Mode", ["Generate Synthetic Patient", "Manual Entry"])

    if input_mode == "Generate Synthetic Patient":
        if st.sidebar.button("Generate New Patient", type="primary"):
            st.session_state['current_patient'] = generate_synthetic_patient()
            st.sidebar.success("Synthetic patient generated!")
    else:
        st.sidebar.subheader("Manual Patient Entry")
        manual_patient = {
            'patient_id': st.sidebar.text_input("Patient ID", "MANUAL_001"),
            'age': st.sidebar.slider("Age", 20, 100, 65),
            'gender': st.sidebar.selectbox("Gender", ["Male", "Female"]),
            'has_hypertension': st.sidebar.checkbox("Hypertension"),
            'has_diabetes': st.sidebar.checkbox("Diabetes"),
            'has_heart_disease': st.sidebar.checkbox("Heart Disease"),
            'has_arrhythmia': st.sidebar.checkbox("Arrhythmia History"),
        }

        if st.sidebar.button("Generate Waveform for Manual Entry", type="primary"):
            manual_patient['segment'] = generate_realistic_waveform(
                manual_patient['age'],
                manual_patient['gender'],
                manual_patient['has_arrhythmia'],
                manual_patient['has_heart_disease']
            )
            manual_patient['chadsvasc_score'] = calculate_chadsvasc_score(
                manual_patient['age'],
                manual_patient['gender'],
                manual_patient['has_hypertension'],
                manual_patient['has_diabetes'],
                manual_patient['has_heart_disease'],
                False,
                manual_patient['has_arrhythmia']
            )
            st.session_state['current_patient'] = manual_patient
            st.sidebar.success("Manual patient data generated!")

    # Main content
    if 'current_patient' not in st.session_state:
        st.info("ğŸ‘ˆ Generate a synthetic patient or enter manual data to begin analysis")

        # Show system info
        col1, col2, col3 = st.columns(3)
        col1.metric("Model Parameters", f"{sum(p.numel() for p in data['model'].parameters()):,}")
        col2.metric("Training Samples", f"{data['dataset_shape'][0]:,}")
        col3.metric("Embedding Dimension", data['config']['embedding_dim'])

        return

    patient = st.session_state['current_patient']

    # Analyze button
    if st.button("ğŸ”¬ Run AI Analysis with Explainability", type="primary", use_container_width=True):
        with st.spinner("Analyzing ECG/PPG signals with explainable AI..."):
            # Prepare input
            segment_tensor = torch.FloatTensor(patient['segment']).transpose(0, 1).unsqueeze(0)

            # Get explanation with signal analysis
            explanation = xai.explain_prediction(segment_tensor, patient['segment'])

            # Predict risk
            risk_prediction = predict_stroke_risk(patient, explanation, data['metadata'])

            # Store in session
            st.session_state['explanation'] = explanation
            st.session_state['risk_prediction'] = risk_prediction

        st.success("âœ… Analysis complete! Scroll down to see comprehensive results.")

    # Display results if available
    if 'explanation' in st.session_state:
        explanation = st.session_state['explanation']
        risk_prediction = st.session_state['risk_prediction']

        # Patient info
        st.header("ğŸ“‹ Patient Information")
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Patient ID", patient['patient_id'])
        col2.metric("Age", f"{patient['age']} years")
        col3.metric("Gender", patient['gender'])
        col4.metric("CHAâ‚‚DSâ‚‚-VASc", patient['chadsvasc_score'])

        # Risk assessment
        st.header("âš ï¸ Stroke Risk Assessment")

        risk_class = f"metric-card risk-{risk_prediction['color']}"
        st.markdown(f"""
        <div class="{risk_class}">
            <h2 style="margin:0;">{risk_prediction['category']}</h2>
            <h1 style="margin:0; color:{risk_prediction['color']};">{risk_prediction['combined_risk']:.1f}%</h1>
            <p style="margin:0;">Combined Stroke Risk Score</p>
        </div>
        """, unsafe_allow_html=True)

        # XAI visualizations
        st.header("ğŸ§  Explainable AI Analysis")

        fig_waveform, fig_channel, fig_risk, fig_embedding = create_xai_visualizations(
            patient, explanation, risk_prediction
        )

        # Explanation boxes
        st.markdown("""
        <div class="explanation-box">
            <h4>ğŸ” How the AI Model Works:</h4>
            <ol>
                <li><b>Signal Processing:</b> Analyzes 8-channel ECG/PPG waveforms (10 seconds, 1250 samples)</li>
                <li><b>Feature Extraction:</b> Deep learning model extracts 256 meaningful features</li>
                <li><b>Pattern Recognition:</b> Identifies cardiovascular patterns learned from thousands of patients</li>
                <li><b>Risk Calculation:</b> Combines AI features with clinical scores (CHAâ‚‚DSâ‚‚-VASc) for comprehensive risk</li>
            </ol>
        </div>
        """, unsafe_allow_html=True)

        st.plotly_chart(fig_waveform, use_container_width=True)

        st.markdown("""
        <div class="explanation-box">
            <h4>ğŸ“Š Temporal Saliency (Red overlay):</h4>
            <p>Shows <b>which time points</b> the AI model focused on to make its prediction.
            Peaks indicate critical moments where abnormal patterns were detected.</p>
        </div>
        """, unsafe_allow_html=True)

        col1, col2 = st.columns(2)

        with col1:
            st.plotly_chart(fig_channel, use_container_width=True)
            st.markdown("""
            <div class="explanation-box">
                <h4>ğŸ“¡ Channel Importance:</h4>
                <p>Shows which signal channels contributed most to the prediction.
                ECG channels (blue) capture electrical activity, PPG channels (orange) capture blood flow.</p>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.plotly_chart(fig_risk, use_container_width=True)
            st.markdown("""
            <div class="explanation-box">
                <h4>âš–ï¸ Risk Components:</h4>
                <p><b>Clinical:</b> Traditional medical risk factors<br>
                <b>Embedding:</b> AI-detected patterns in waveforms<br>
                <b>Age:</b> Age-related risk adjustment</p>
            </div>
            """, unsafe_allow_html=True)

        st.plotly_chart(fig_embedding, use_container_width=True)

        st.markdown("""
        <div class="explanation-box">
            <h4>ğŸ¯ Embedding Features:</h4>
            <p>The AI model transforms raw signals into 256 learned features.
            Red bars show highly activated features that influenced the risk prediction.</p>
        </div>
        """, unsafe_allow_html=True)

        # Comprehensive Clinical and AI Findings
        st.markdown('<div class="section-header">ğŸ“ Comprehensive Clinical & AI Findings</div>', unsafe_allow_html=True)

        # Signal Analysis Results
        st.subheader("ğŸ”¬ Signal Analysis Results")

        if 'signal_analysis' in explanation:
            # Create a detailed table of metrics
            signal_data = []
            for ch_name, metrics in explanation['signal_analysis'].items():
                ch_num = int(ch_name.replace('ch', '')) + 1
                ch_type = "ECG-like" if ch_num <= 3 else "PPG-like"
                signal_data.append({
                    'Channel': f'Ch {ch_num} ({ch_type})',
                    'Heart Rate (bpm)': f"{metrics['hr']:.1f}" if metrics['hr'] > 0 else "N/A",
                    'HRV': f"{metrics['hrv']:.3f}",
                    'Peaks Detected': metrics['peaks'],
                    'Irregular Beats': metrics['irregular_beats'],
                    'Amplitude': f"{metrics['amplitude']:.2f}"
                })

            df_signals = pd.DataFrame(signal_data)
            st.dataframe(df_signals, use_container_width=True, hide_index=True)

        # Abnormalities Detection
        if 'abnormalities' in explanation and explanation['abnormalities']:
            st.subheader("âš ï¸ Detected Abnormalities")
            for abn in explanation['abnormalities']:
                st.markdown(f"""
                <div class="finding-box">
                    <strong>ğŸ”´ {abn}</strong>
                </div>
                """, unsafe_allow_html=True)
        else:
            st.success("âœ… No significant abnormalities detected in waveform analysis")

        # AI Model Insights
        st.subheader("ğŸ¤– AI Model Insights")

        col1, col2, col3 = st.columns(3)

        with col1:
            top_channel = np.argmax(explanation['channel_importance']) + 1
            st.metric(
                "Most Informative Channel",
                f"Channel {top_channel}",
                f"{explanation['channel_importance'].max()*100:.1f}% contribution"
            )

        with col2:
            critical_time = np.argmax(explanation['temporal_saliency']) / 125
            st.metric(
                "Critical Time Point",
                f"{critical_time:.2f}s",
                "Peak AI attention"
            )

        with col3:
            high_activation = (np.abs(explanation['embedding'][0]) > 0.5).sum()
            st.metric(
                "Feature Activation",
                f"{high_activation}/256",
                f"{high_activation/256*100:.1f}%"
            )

        # Clinical Risk Factors
        st.subheader("âš•ï¸ Clinical Risk Assessment")

        findings = []

        # Risk factors
        if patient['chadsvasc_score'] >= 2:
            findings.append({
                'icon': 'ğŸ”´',
                'title': 'Elevated CHAâ‚‚DSâ‚‚-VASc Score',
                'detail': f"Score of {patient['chadsvasc_score']} indicates increased baseline stroke risk"
            })

        if patient['has_hypertension']:
            findings.append({
                'icon': 'âš ï¸',
                'title': 'Hypertension Present',
                'detail': 'Known cardiovascular risk factor'
            })

        if patient['has_diabetes']:
            findings.append({
                'icon': 'âš ï¸',
                'title': 'Diabetes Mellitus',
                'detail': 'Increases vascular complications risk'
            })

        if patient['has_heart_disease']:
            findings.append({
                'icon': 'âš ï¸',
                'title': 'Pre-existing Heart Disease',
                'detail': 'Significant cardiovascular history'
            })

        if patient['has_arrhythmia']:
            findings.append({
                'icon': 'ğŸ”´',
                'title': 'Arrhythmia History',
                'detail': 'Major risk factor for stroke'
            })

        if patient['age'] > 75:
            findings.append({
                'icon': 'ğŸŸ¡',
                'title': 'Advanced Age',
                'detail': f'Age {patient["age"]} increases baseline risk'
            })

        if risk_prediction['embedding_risk'] > 60:
            findings.append({
                'icon': 'ğŸ¤–',
                'title': 'AI-Detected Abnormal Patterns',
                'detail': f'Model confidence: {risk_prediction["embedding_risk"]:.1f}%'
            })

        if findings:
            for finding in findings:
                st.markdown(f"""
                <div class="finding-box">
                    <span style="font-size: 20px;">{finding['icon']}</span>
                    <strong>{finding['title']}</strong><br>
                    <span style="color: #666;">{finding['detail']}</span>
                </div>
                """, unsafe_allow_html=True)
        else:
            st.info("âœ… No major clinical risk factors identified")

        # Generate report
        st.header("ğŸ“„ Generate Report")

        if st.button("Generate PDF Report", type="secondary", use_container_width=True):
            with st.spinner("Generating comprehensive PDF report..."):
                pdf_buffer = generate_pdf_report(
                    patient, explanation, risk_prediction,
                    [fig_waveform, fig_channel, fig_risk, fig_embedding]
                )

            st.download_button(
                label="ğŸ“¥ Download PDF Report",
                data=pdf_buffer,
                file_name=f"ECG_Analysis_{patient['patient_id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                mime="application/pdf",
                use_container_width=True
            )
            st.success("Report generated! Click above to download.")


if __name__ == "__main__":
    main()
